{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20220809(final data로 BERT 모델링  진행).ipynb","provenance":[{"file_id":"1itzUVL6GbRXeiPG2OrTIxeteFs9qZrCP","timestamp":1660029312390},{"file_id":"1CzsV3fwJR9HG34gKHmenhHN-FbZeKsNg","timestamp":1659595140610},{"file_id":"1DSzQOZ2TAdA-qNrp_3awAgU7mWtsFvRY","timestamp":1659509662271},{"file_id":"1RFHuFj6RR08vxkxNrR8-FTwkP9Zm9FM5","timestamp":1659003324943}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"8f831cc9a35146ed95c7792b038690db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4f764457829485099c9e677ef5c2fc9","IPY_MODEL_2ef14436cdc34463ace29ddff4b25d86","IPY_MODEL_6bca54049fd541e988d85d3b3615512e"],"layout":"IPY_MODEL_6c6e823804d1468d8e79f7dee3f48937"}},"f4f764457829485099c9e677ef5c2fc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0ee15ac7b8e42049639e9bda6e3a949","placeholder":"​","style":"IPY_MODEL_480e02d30b8041fda0cb0ca40231f7a2","value":"Downloading: 100%"}},"2ef14436cdc34463ace29ddff4b25d86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a275b5fb0f324bb4b43148e34d67697f","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c482af3bd5a444e3b9104368a868985f","value":995526}},"6bca54049fd541e988d85d3b3615512e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b524b35eb6549dbbdbeb825806aa2af","placeholder":"​","style":"IPY_MODEL_d51d08d16c284a3b9e1e8f8a21d33ca2","value":" 996k/996k [00:00&lt;00:00, 1.16MB/s]"}},"6c6e823804d1468d8e79f7dee3f48937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0ee15ac7b8e42049639e9bda6e3a949":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"480e02d30b8041fda0cb0ca40231f7a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a275b5fb0f324bb4b43148e34d67697f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c482af3bd5a444e3b9104368a868985f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b524b35eb6549dbbdbeb825806aa2af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d51d08d16c284a3b9e1e8f8a21d33ca2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## 0. 구글 드라이브 연결 및 필수 라이브러리 설치 및 로드"],"metadata":{"id":"2kssQGB5iToK"}},{"cell_type":"markdown","source":["### 구글 드라이브 연결"],"metadata":{"id":"dt51lRmEzT4D"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07aeCwO3iTbc","executionInfo":{"status":"ok","timestamp":1660055795049,"user_tz":-540,"elapsed":16850,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"0e00ad33-e46d-4800-add0-fee3c2f140ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["### 필수 라이브러리 설치 및 로드"],"metadata":{"id":"gjjX5duczWCV"}},{"cell_type":"code","source":["#!pip install nltk"],"metadata":{"id":"MIoFlaitQuS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install konlpy"],"metadata":{"id":"ly6O3vlRH93D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from konlpy.tag import Okt\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","#import nltk\n","import re\n","\n","from keras.callbacks import *\n","import os\n","import tensorflow as tf"],"metadata":{"id":"g35dYsbyH7k8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#nltk.download()\n","\n","# 이거 안하면 lookuperror 뜬다.(필요한 것만 지정해서 다운로드 해도되고, all을 사용하여 모두 다운로드 해도 된다.)"],"metadata":{"id":"Mw1S6BjNRZqn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이후에 데이터 및 파일들을 저장할 경로를 path로 통일하여 따로 저장\n","\n","path = '/content/gdrive/MyDrive/Colab Notebooks/CUAI 하계프로젝트-NLP/초기 모델링 진행/data/'"],"metadata":{"id":"hMgs4OigdbWe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***여기 이후부터는 완료된 단계(4. 토큰화, 5. 정수 인코딩 등)로 점프해서 실행시키면 됨!!(노트북을 나누면서 앞부분 삭제함)***"],"metadata":{"id":"kGn6QmC1c61X"}},{"cell_type":"markdown","source":["## 6. BERT 모델링"],"metadata":{"id":"Pq73212r69Dv"}},{"cell_type":"markdown","source":["### 6-1. BERT 모델링에 필요한 모듈 및 함수 import  "],"metadata":{"id":"k9UP-iC5WQma"}},{"cell_type":"markdown","source":["앞선 단계에서 토큰화, 정수 인코딩, 패딩, 워드 임베딩을 수행했으나   \n","BERT 모델링에 참고한 책 **\"텐서플로 2와 머신러닝으로 시작하는 자연어 처리\"**를 보니(p.368~371) 기존에 한 것과 아예 다른 토크나이저를 사용하기 때문에 모델의 입력 데이터를 구성하는 방식이 다르다고 함  \n","따라서 책에 있는 코드를 따라가며 BERT 모델링에 필요한 모듈 및 함수 import하였음"],"metadata":{"id":"BtKFRnW-ljM5"}},{"cell_type":"code","source":["!pip install transformers==3.0.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JtIgB9C0qWWG","executionInfo":{"status":"ok","timestamp":1660055804590,"user_tz":-540,"elapsed":7083,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"19eda856-a3c3-4cb3-f14b-388583c506d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.0.2\n","  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n","\u001b[K     |████████████████████████████████| 769 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 60.4 MB/s \n","\u001b[?25hCollecting tokenizers==0.8.1.rc1\n","  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 51.0 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 65.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ed04480c53cdd9398245014a4c62d141fc916e2e9aaeec894bc289a412952878\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.1rc1 transformers-3.0.2\n"]}]},{"cell_type":"code","source":["# 모델 학습에 필요한 모듈 import 및 버트의 다국어 토크나이저 다운로드\n","from transformers import *\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"],"metadata":{"id":"FCnSb8Kf-LA-","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["8f831cc9a35146ed95c7792b038690db","f4f764457829485099c9e677ef5c2fc9","2ef14436cdc34463ace29ddff4b25d86","6bca54049fd541e988d85d3b3615512e","6c6e823804d1468d8e79f7dee3f48937","b0ee15ac7b8e42049639e9bda6e3a949","480e02d30b8041fda0cb0ca40231f7a2","a275b5fb0f324bb4b43148e34d67697f","c482af3bd5a444e3b9104368a868985f","5b524b35eb6549dbbdbeb825806aa2af","d51d08d16c284a3b9e1e8f8a21d33ca2"]},"executionInfo":{"status":"ok","timestamp":1660055808143,"user_tz":-540,"elapsed":3557,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"d8ab2950-ca38-44c7-ca67-f494ff59a28c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f831cc9a35146ed95c7792b038690db"}},"metadata":{}}]},{"cell_type":"markdown","source":["### 6-2. 버트 문장 전처리 진행하기"],"metadata":{"id":"Ztjbt7g3gfqg"}},{"cell_type":"markdown","source":["버트 모델은 일반적으로 3가지 입력값을 받게 되는데, 요약하면 아래 표와 같음\n","\n","|입력값|역할|\n","|-------|--------|\n","| input_ids | 문장을 토크나이즈해서 인덱스 값으로 변환한다. 일반적으로 버트에서는 단어를 서브워드 단위로 변환시키는 워드피스 토크나이저를 활용한다.  \n","| attention_mask |어텐션 마스크는 패딩된 부분에 대해 학습에 영향을 받지 않기 위해 처리해주는 입력값이다. 버트 토크나이저에서 1은 어텐션에 영향을 받는 토큰을, 0은 영향을 받지 않는 토큰을 나타낸다.|\n","|token_type_ids|두 개의 시퀀스를 입력으로 활용할 때 0과 1로 문장의 토큰값을 분리한다.|"],"metadata":{"id":"usDCPqUziFJk"}},{"cell_type":"markdown","source":["버트 토크나이저에서는 문장의 시작이나 끝, 모델별 목적에 맞게 스페셜 토큰을 활용함  \n","다음은 자주 사용되는 스페셜 토큰들을 요약한 표임\n","\n","|스페셜 토큰|역할|\n","|-------|--------|\n","|[UNK]|모르는 단어에 대한 토큰|\n","|[MASK]|마스크 토큰, 사전 학습에서 활용|\n","|[PAD]|최대 길이를 맞추는 용도| \n","|[SEP]|문장의 종결을 알림| \n","|[CLS]|문장의 시작을 알림|"],"metadata":{"id":"dsTDVOdjjA5D"}},{"cell_type":"markdown","source":["버트에 필요한 입력값의 형태로 데이터를 변환하는 구조를 직접 구현할 수도 있지만, **허깅페이스의 Tokenizer** 라이브러리를 활용하면 더 쉽고 빠르게 버트의 입력값을 구현할 수 있음  \n","\n","참고한 책(\"텐서플로 2와 머신러닝으로 시작하는 자연어 처리\")에서는 이 라이브러리 중 버트 토크나이저 작업에 필요한 구조를 만들어주는 **encode_plus** 기능을 활용하였음  \n","- **encode_plus** 기능: 특정 문장을 버트에 필요한 입력 형태로 변환하는 것뿐만 아니라 문장을 최대 길이에 맞게 패딩까지 해주며, 결과값은 딕셔너리 형태로 출력됨  \n","\n","변환 순서는 아래 코드와 같음"],"metadata":{"id":"G22dBR96jm7y"}},{"cell_type":"code","source":["def bert_tokenizer(sent, MAX_LEN): \n","\n","    encoded_dict = tokenizer.encode_plus(\n","        text = sent, # 이 부분이 원래 sent1, sent2라고 되어있었는데 뒤에서 not defined 떠서 바꿈\n","        add_special_tokens = True,    # Add '[CLS]' and '[SEP]'\n","        max_length = MAX_LEN,         # Pad & truncate all sentences\n","        pad_to_max_length = True, \n","        return_attention_mask = True, # Construct attention masks\n","        truncation=True\n","    )\n","\n","    input_id = encoded_dict['input_ids']\n","    attention_mask = encoded_dict['attention_mask'] # simply differentiates padding from non-padding\n","    token_type_id = encoded_dict['token_type_ids']  # differentiate two sentences\n","\n","    return input_id, attention_mask, token_type_id"],"metadata":{"id":"dSr5hIWQgqjm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**encode_plus**의 변환 순서는 다음과 같음\n","1. 문장을 토크나이징함\n","2. add_special_tokens를 True로 지정하면 토큰의 시작점에 '[CLS]' 토큰, 토큰의 마지막에 '[SEP]' 토큰을 붙임\n","3. 각 토큰을 인덱스로 변환함\n","4. max_length에 MAX_LEN 최대 길이에 따라 문장의 길이를 맞추는 작업을 진행하고, pad_to_max_length 기능을 통해 MAX_LEN의 길이에 미치지 못하는 문장에 패딩을 적용함\n","5. return_attention_mask 기능을 통해 어텐션 마스크를 생성함\n","6. 토큰 타입은 문장이 1개일 경우 0으로, 문장이 2개일 경우 0과 1로 구분해서 생성함"],"metadata":{"id":"GbV0si7npY-Q"}},{"cell_type":"code","source":["# 간단한 예시로 토크나이저가 잘 동작하는지 확인\n","\n","# Special Tokens\n","print(tokenizer.all_special_tokens, '\\n', tokenizer.all_special_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzA5_JzOgqgE","executionInfo":{"status":"ok","timestamp":1660056466937,"user_tz":-540,"elapsed":283,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"4c566d32-a8e3-43ad-f9b4-4e64c7c45436"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', '[MASK]', '[SEP]', '[PAD]', '[UNK]'] \n"," [101, 103, 102, 0, 100]\n"]}]},{"cell_type":"code","source":["# Test Tokenizers\n","kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다\")\n","eng_encode = tokenizer.encode(\"Hello world\")\n","kor_decode = tokenizer.decode(kor_encode)\n","eng_decode = tokenizer.decode(eng_encode)\n","\n","print(kor_encode)\n","print(eng_encode)\n","print(kor_decode)\n","print(eng_decode)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLgJkvb4gqck","executionInfo":{"status":"ok","timestamp":1660056468173,"user_tz":-540,"elapsed":2,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"8492ff32-f898-42bc-92f1-a4888df4da98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 102]\n","[101, 31178, 11356, 102]\n","[CLS] 안녕하세요, 반갑습니다 [SEP]\n","[CLS] Hello world [SEP]\n"]}]},{"cell_type":"markdown","source":["### 6-3. final_data를 train/test로 나눈 후 train 데이터 전처리"],"metadata":{"id":"z6Rdy9r6tsTO"}},{"cell_type":"code","source":["# 후에 나올 주요 변수들 정의\n","MAX_LEN = 25 # okt 형태소 분석기로 분석했을 때 95%의 데이터들이 살아남은 지점의 길이\n","NUM_EPOCHS = 3 # 임의로 설정\n","BATCH_SIZE = 64 # 임의로 설정\n","VALID_SPLIT = 0.1 # 임의로 설정"],"metadata":{"id":"hLmu8ru8AyHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# final_data 로드\n","# (참고)path 부분은 맨 앞 0장의 마지막 코드에서 경로 지정하였음!\n","\n","data = pd.read_excel(path + 'final_data.xlsx', engine='openpyxl')"],"metadata":{"id":"5rFcqlqyWQBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.tail(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"6EvyQZMRW8II","executionInfo":{"status":"ok","timestamp":1660055830096,"user_tz":-540,"elapsed":13,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"8a7e556d-2348-4b7a-a516-c91e63692a7d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        Unnamed: 0                       document  label\n","303472      303472       당장 내일 밥은 어떡하고 내 병원비는 어쩌지      0\n","303473      303473  응 이제 다들 독립해가지고 명절 아니면 찾아오질 않아      0\n","303474      303474               안부전화라도 해주면 좋을 텐데      0"],"text/html":["\n","  <div id=\"df-db40caca-1775-42dc-8b1d-7d6212d8b7f8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>303472</th>\n","      <td>303472</td>\n","      <td>당장 내일 밥은 어떡하고 내 병원비는 어쩌지</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>303473</th>\n","      <td>303473</td>\n","      <td>응 이제 다들 독립해가지고 명절 아니면 찾아오질 않아</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>303474</th>\n","      <td>303474</td>\n","      <td>안부전화라도 해주면 좋을 텐데</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db40caca-1775-42dc-8b1d-7d6212d8b7f8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-db40caca-1775-42dc-8b1d-7d6212d8b7f8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-db40caca-1775-42dc-8b1d-7d6212d8b7f8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# 책에 train과 test 데이터를 나눴으므로 train_test_split을 이용해 final_data를 분리\n","# test_size = 0.25가 defalut, 랜덤 시드 42로 고정\n","X_train, X_test, y_train, y_test = train_test_split(data['document'],data['label'], \n","                                                    test_size = 0.25, stratify = data['label'], random_state=42)"],"metadata":{"id":"n3gQJ_cRt6Oo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 다시 train과 test끼리 묶어주고 이것을 일단 txt 파일로 저장\n","train_data = pd.concat([X_train, y_train], axis=1)\n","test_data = pd.concat([X_test, y_test], axis=1)\n","\n","train_data.to_csv(path + 'final_train_data.txt', sep = '\\t', index = False)\n","test_data.to_csv(path + 'final_test_data.txt', sep = '\\t', index = False)"],"metadata":{"id":"3xq2K81nt6Lm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6-2에서 만든 토크나이저와 encode_plus 기능을 활용해 데이터를 전처리"],"metadata":{"id":"24l7K51cs-UW"}},{"cell_type":"markdown","source":["아래 코드 중 train_data_sent_pads 부분 짤 때 참고한 사이트: https://github.com/changwookjun/lgsp_nlp_practice/blob/master/Bert_Naver_movie_tf_mentee.ipynb"],"metadata":{"id":"S1qCKJWrjnUu"}},{"cell_type":"code","source":["input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","train_data_labels = []\n","\n","def clean_text(sent):\n","    sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \" \", sent)\n","    return sent_clean\n","\n","for train_sent, train_label in train_data[['document', 'label']].values: \n","    try:\n","        # 아래 코드에 MAX_LEN에도 숫자를 넣어줘야 할 것 같음\n","        # 여기서는 버트 토크나이저의 word piece model을 통해 파악한 토큰 길이 분포 중\n","        # 제3사분위 값인 39를 정했다고 함\n","        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), MAX_LEN)\n","\n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","        train_data_labels.append(train_label)\n","    \n","    except Exception as e:\n","        print(e)\n","        print(train_sent)\n","        pass\n","\n","\n","train_input_ids = np.array(input_ids, dtype=int)\n","train_attention_masks = np.array(attention_masks, dtype=int)\n","train_type_ids = np.array(token_type_ids, dtype=int)\n","train_inputs = (train_input_ids, train_attention_masks, train_type_ids)\n","\n","# 책에서는 이 변수 정의한 부분이 없었는데 정작 학습에서는 등장해서 구글링으로 대충 찾아봄\n","train_data_sent_pads = pad_sequences(train_input_ids, maxlen=MAX_LEN, padding='pre')\n","train_data_labels = np.asarray(train_data_labels, dtype=np.int32) # 정답 토크나이징 리스트\n","\n","print(\"# sents: {}, # labels: {}\".format(len(train_input_ids), len(train_data_labels)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dK1rjbC8tIDr","executionInfo":{"status":"ok","timestamp":1660056562430,"user_tz":-540,"elapsed":55871,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"1c76c310-e8dd-48d1-c9ed-cb2022b85920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# sents: 227606, # labels: 227606\n"]}]},{"cell_type":"markdown","source":["위 전처리 과정 설명  \n","- 우선 데이터에서 한글 외 텍스트를 제거하기 위한 clean_text 함수를 적용\n","- 버트 토크나이저를 활용한 인코딩 진행\n","- 최종 출력값은 넘파이로 변환한 후 train_inputs에 튜플 형태로 묶어서 저장하는 형태로 구성"],"metadata":{"id":"FPQeYTP-vsni"}},{"cell_type":"code","source":["# 원하면 책 p.373에 있는 코드로 예시 확인해 볼 수 있게 남겨둔 여백"],"metadata":{"id":"OjB_8eq2wgdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6-4. final_train_data를 모델에 학습시키기"],"metadata":{"id":"AslnGvWO_Y15"}},{"cell_type":"code","source":["# 학습을 준비하기 위해 버트 분류 클래스를 구성하고, 최적화, 손실값, 및 정확도를 선언하고 학습을 진행하기 위한 compile을 진행\n","class TFBertClassifier(tf.keras.Model):\n","    def __init__(self, model_name, dir_path, num_class):\n","        super(TFBertClassifier, self).__init__()\n","\n","        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n","        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n","        self.classifier = tf.keras.layers.Dense(\n","            num_class,\n","            kernel_initializer = tf.keras.initializers.TruncatedNormal(\n","                self.bert.config.initializer_range\n","            ),\n","            name=\"classifier\"\n","        )\n","    \n","    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n","\n","        # outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n","        outputs = self.bert(\n","            inputs, attention_mask=attention_mask, token_type_ids=token_type_ids\n","        )\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output, training=training)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased', \n","                             dir_path='bert_ckpt', \n","                             num_class=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KQby6rMwz5z","executionInfo":{"status":"ok","timestamp":1660056766380,"user_tz":-540,"elapsed":1830,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"1915c8e4-fc53-4cb1-8e67-ae94c5482a9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["TFBertClassifier 클래스 설명  \n",": 이 클래스는 사전 학습된 버트 모델을 불러와 그 위에 완전연결층 1층을 쌓는 구조로 되어 있음  \n","\n","\\__init__ 부분\n","- model_name, dir_path, num_class를 인자로 받아 활용할 모델 이름, 모델이 저장된 위치, 클래스의 수를 모델에 지정함\n","- self.bert 부분의 TFBertModel.from_pretrained를 통해 기존에 사전 학습했던 부분이 로드됨\n","- TFBertModel의 결과값은 sequence_output, pooled_output, hidden_states, attentions로 구성됨\n","- num_class에 원하는 분류 값을 추가해서 정답의 개수를 정할 수 있음, 감정 분석 데이터에서는 정답이 2개(긍정, 부정)으로 나뉨  \n","\n","call 메소드 부분  \n","- inputs를 통해 문장 텍스트를 받은 다음,\n","- 버트에서 outputs로 결과값을 추출한 다음,\n","- self.classifier를 통해 완전 연결층을 활용해 최종적으로 self.num_labels 개수에 맞는 예측값을 출력하는 구조  \n","\n","이 구조에서 핵심은 사전 학습된 부분을 불러와서 그 이후에 다양한 종류의 자연어 문제에 적용하는 것임"],"metadata":{"id":"FrsrwTkXzL5V"}},{"cell_type":"code","source":["# 앞에서 구성한 TFBertClassifier를 활용해 최적화, 손실값 및 평가 기준을 설정하는 과정\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","cls_model.compile(optimizer=optimizer,\n","                  loss=loss,\n","                  metrics=[metric])"],"metadata":{"id":"0o2xuVgh1L1s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 최적화: 아담 최적화 사용\n","- 손실값: SparseCategoricalCrossentropy 사용\n","- 평가 기준: 모델의 정확도를 측정하는 SparseCategoricalAccuracy를 정의  \n","\n","위 기준을 모델의 compile 함수에 넣어 모델을 학습할 준비를 모두 마침"],"metadata":{"id":"jYVVgxbv1fHu"}},{"cell_type":"markdown","source":["학습시키기 전에 GPU 활용을 위한 코드를 삽입해보았음  \n","참고 사이트: https://m.blog.naver.com/demian7607/222043724449 "],"metadata":{"id":"TTFNLzoGZ3nl"}},{"cell_type":"code","source":["# 1) GPU 디바이스 확인\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqNr8T7u1BLh","executionInfo":{"status":"ok","timestamp":1660056810784,"user_tz":-540,"elapsed":288,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"8a71fca8-5d09-4e70-d547-105a51320d06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["# 이제 학습을 시작\n","# 워낙 큰 파라미터를 사용하기 때문에 CPU에서의 학습은 거의 불가능\n","# 따라서 웬만하면 반드시 GPU를 활용해야 함\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n","\n","# 이부분 경로는 일단 그냥 구글드라이브의 data 모아놨던 곳으로 코드 바꿈\n","# model_name 자리에도 위에 적혀있던 'bert-base-multilingual-cased' 적어놓음\n","checkpoint_path = os.path.join(path, 'bert-base-multilingual-cased', 'weights.h5')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","else: \n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","cp_callback = ModelCheckpoint(\n","    checkpoint_path, \n","    monitor=\"val_accuracy\", \n","    verbose=1, \n","    save_best_only=True, \n","    save_weights_only=True, \n",")\n","\n","\n","with tf.device('/device:GPU:0'):\n","    history = cls_model.fit(\n","        train_data_sent_pads, # 책에는 train_data_sent_pads로 되어있는데 어디에도 이렇게 정의한 변수가 없음\n","        train_data_labels, \n","        epochs=NUM_EPOCHS, # 앞에 6-3에서 임의로 정함\n","        batch_size=BATCH_SIZE, # 앞에 6-3에서 임의로 정함\n","        validation_split=VALID_SPLIT, # 앞에 6-3에서 임의로 정함\n","        callbacks=[earlystop_callback, cp_callback]\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXyekWhe2Xoj","executionInfo":{"status":"ok","timestamp":1660060603716,"user_tz":-540,"elapsed":3646365,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"781af2d3-9f63-4763-8abf-53d29fa65431"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/CUAI 하계프로젝트-NLP/초기 모델링 진행/data/bert-base-multilingual-cased -- Folder already exists \n","\n","Epoch 1/3\n","3201/3201 [==============================] - ETA: 0s - loss: 0.6659 - accuracy: 0.6230\n","Epoch 1: val_accuracy improved from -inf to 0.62550, saving model to /content/gdrive/MyDrive/Colab Notebooks/CUAI 하계프로젝트-NLP/초기 모델링 진행/data/bert-base-multilingual-cased/weights.h5\n","3201/3201 [==============================] - 1221s 381ms/step - loss: 0.6659 - accuracy: 0.6230 - val_loss: 0.6623 - val_accuracy: 0.6255\n","Epoch 2/3\n","3201/3201 [==============================] - ETA: 0s - loss: 0.6686 - accuracy: 0.6196\n","Epoch 2: val_accuracy did not improve from 0.62550\n","3201/3201 [==============================] - 1213s 379ms/step - loss: 0.6686 - accuracy: 0.6196 - val_loss: 0.6628 - val_accuracy: 0.6255\n","Epoch 3/3\n","3201/3201 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.6256\n","Epoch 3: val_accuracy did not improve from 0.62550\n","3201/3201 [==============================] - 1212s 379ms/step - loss: 0.6628 - accuracy: 0.6256 - val_loss: 0.6613 - val_accuracy: 0.6255\n"]}]},{"cell_type":"markdown","source":["### 6-4. final_test_data로 모델의 성능 테스트하기"],"metadata":{"id":"K_1voRDyZwXh"}},{"cell_type":"code","source":["# 이제 test 데이터로 테스트를 진행\n","input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","test_data_labels = []\n","\n","for test_sent, test_label in zip(test_data['document'], test_data['label']): \n","    try:\n","        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(test_sent), MAX_LEN)\n","\n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","        test_data_labels.append(test_label)\n","    \n","    except Exception as e:\n","        print(e)\n","        print(test_sent)\n","        pass\n","\n","test_input_ids = np.array(input_ids, dtype=int)\n","test_attention_masks = np.array(attention_masks, dtype=int)\n","test_type_ids = np.array(token_type_ids, dtype=int)\n","test_inputs = (test_input_ids, test_attention_masks, test_type_ids)\n","\n","test_data_labels = np.asarray(test_data_labels, dtype=np.int32) # 정답 토크나이징 리스트\n","\n","print(\n","    \"num sents, labels {}, {}\".format(len(test_input_ids), len(test_data_labels))\n",")\n","\n","results = cls_model.evaluate(test_inputs, test_data_labels, batch_size=512)\n","print(\"test loss, test acc:\", results)"],"metadata":{"id":"JXmhZ8F5ZpqX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660060774707,"user_tz":-540,"elapsed":134837,"user":{"displayName":"이강민","userId":"04405449783523250799"}},"outputId":"5938c33e-8e80-403c-97a8-4c1dfb3c2f56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num sents, labels 75869, 75869\n","149/149 [==============================] - 117s 767ms/step - loss: 0.6613 - accuracy: 0.6256\n","test loss, test acc: [0.6612667441368103, 0.6256178617477417]\n"]}]},{"cell_type":"markdown","source":["test_data에 모델을 테스트해본 결과 accuracy가 62.56%로 굉장히 나쁜 성능을 보였음  \n","그전에 썼던 GPT2나 다른 KoBERT, Bi-LSTM 모델의 결과와 비교해 본 뒤 가장 성능이 좋은 모델을 선택하면 될 것 같음"],"metadata":{"id":"iUxl_ufslHNe"}},{"cell_type":"markdown","source":["# END"],"metadata":{"id":"bBhlR9eulbfo"}}]}
