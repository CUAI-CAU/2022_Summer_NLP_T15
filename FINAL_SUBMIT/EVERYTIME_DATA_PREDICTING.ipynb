{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-tblwjkCvYt"
      },
      "source": [
        "# 0. 구글 드라이브 연동 및 필수 라이브러리 설치 및 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPlR7IH6C0X7"
      },
      "source": [
        "### 구글 드라이브 연동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rWOSsfdNAwkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d827c4-dbb5-4b1e-e57b-ccec2c157542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHau5dOtC2fU"
      },
      "source": [
        "### 필수 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_dObIMhC5Zv",
        "outputId": "b0bf09a1-eb80-404a-943b-e0d02a3f470f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[K     |████████████████████████████████| 416 kB 30.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ]
        }
      ],
      "source": [
        "!pip install soynlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww-iaPjIsbqN",
        "outputId": "3adf4aa3-b4a7-4ae9-f4d8-647efaf790a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Si4xHNjslec",
        "outputId": "41de9319-f7bf-4230-f5a7-9e3f02e8ceab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.4.0-py3-none-any.whl (985 kB)\n",
            "\u001b[K     |████████████████████████████████| 985 kB 34.1 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 61.9 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 61.8 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.4.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.11 wsproto-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP82YRBuD884",
        "outputId": "4139634f-543e-4db6-fa98-e9723141d133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-q7s84f4v\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-q7s84f4v\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 32.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4868 sha256=9315ae97601648b62651297f7a27b685c17b58b834f5a63b97719eaef30ca7d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gqnklqh7/wheels/ab/f5/7b/d4124bb329c905301baed80e2ae45aa14e824f62ebc3ec2cc4\n",
            "Successfully built py-hanspell\n",
            "Installing collected packages: urllib3, py-hanspell\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.11\n",
            "    Uninstalling urllib3-1.26.11:\n",
            "      Successfully uninstalled urllib3-1.26.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.4.0 requires urllib3[secure,socks]~=1.26, but you have urllib3 1.25.11 which is incompatible.\u001b[0m\n",
            "Successfully installed py-hanspell-1.1 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyzkRBpaFZOL",
        "outputId": "0d256a9c-5ccc-45b3-8142-80d160cdfd71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhlSx6FS0rZE",
        "outputId": "a86f316c-35cb-4bc1-8946-f0ad67066702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsJxffwpmmMp",
        "outputId": "98b92f1d-50a5-4668-d8af-83e53e8dcf31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xijzl-ub1pBi",
        "outputId": "85755e42-bd23-4516-e8f1-6ebf634d97fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 28.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.21.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.32)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.9)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595738 sha256=018ad38543843761d8fe1b37c9885cd68e8394bb4186e5f4e4173395268dc935\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gluonnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MyXAdS51pHY",
        "outputId": "07ba392f-a43e-49b1-9285-646d16ddab3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 27.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVluLgmp17-7",
        "outputId": "276440d4-1446-47b1-e963-6cfea4bb87fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=5a934f86f3e2ea1b5cc8147d7327d634ad9cc297b21ed4c5f766a613e7eebf40\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "! pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y09F_F2N2ifD",
        "outputId": "1b13bf78-b513-4fe6-a14d-bb4d70d3801b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym6Oa-EM249h",
        "outputId": "2b0460a0-1718-4c06-e3a6-067b5854ab62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 34.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUpRRSy7Fk-L",
        "outputId": "098b6fae-76ee-4cf9-80e4-ddc29b69f6cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-g0tadscb\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-g0tadscb\n",
            "Collecting boto3<=1.15.18\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 26.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
            "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.7 MB 17 kB/s \n",
            "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 51.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 58.7 MB/s \n",
            "\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n",
            "  Downloading torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.3 MB/s eta 0:00:38tcmalloc: large alloc 1147494400 bytes == 0x3a544000 @  0x7f4a0543c615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 16 kB/s \n",
            "\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (2.0)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 848 kB/s \n",
            "\u001b[?25hCollecting botocore<1.19.0,>=1.18.18\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 58.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.32)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.7.1)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.1.0)\n",
            "Building wheels for collected packages: kobert, sacremoses\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=ecee2b066542b6352f8ab5bcbae36d75e6699a4dda65b6f70d82271ac2a9c8b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7f5sohzy/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e20b6859839b51b9123c54c0dc66663133c0d0c232070a18c9508e3421d05656\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built kobert sacremoses\n",
            "Installing collected packages: jmespath, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, transformers, torch, sentencepiece, onnxruntime, mxnet, boto3, kobert\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.8.1\n",
            "    Uninstalling huggingface-hub-0.8.1:\n",
            "      Successfully uninstalled huggingface-hub-0.8.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.21.1\n",
            "    Uninstalling transformers-4.21.1:\n",
            "      Successfully uninstalled transformers-4.21.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.97\n",
            "    Uninstalling sentencepiece-0.1.97:\n",
            "      Successfully uninstalled sentencepiece-0.1.97\n",
            "  Attempting uninstall: mxnet\n",
            "    Found existing installation: mxnet 1.9.1\n",
            "    Uninstalling mxnet-1.9.1:\n",
            "      Successfully uninstalled mxnet-1.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYgOP9tHGTHK",
        "outputId": "49fa7253-6046-4a0c-91f4-583465cb2ec9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-ng7xfch5/kobert-tokenizer_d05cafaa723c421f8556471fbca4d4a1\n",
            "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-ng7xfch5/kobert-tokenizer_d05cafaa723c421f8556471fbca4d4a1\n",
            "Building wheels for collected packages: kobert-tokenizer\n",
            "  Building wheel for kobert-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4649 sha256=6fb3f8a5873353b24dd95fc27525eddd884b6484c675d67fd552d309145b54a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c_m_2lco/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4\n",
            "Successfully built kobert-tokenizer\n",
            "Installing collected packages: kobert-tokenizer\n",
            "Successfully installed kobert-tokenizer-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGkUXoI4C5c9"
      },
      "source": [
        "### 필수 라이브러리 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1KdvKu2RBz23"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from soynlp.normalizer import *\n",
        "from hanspell import spell_checker\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pickle\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from transformers import TFGPT2LMHeadModel, TFGPT2Model\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.callbacks import *\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sTljHUDV0yWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e833992a-fde6-44ef-af94-7e89e5a9a36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSGPIVDQmVsL"
      },
      "source": [
        "### 폴더 경로 지정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T2RxFLAzmWHF"
      },
      "outputs": [],
      "source": [
        "path = '/content/gdrive/MyDrive/FINAL_SUBMIT/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6mM--UVz1hi"
      },
      "source": [
        "# 1. 필요한 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ULQeOtAz6Gy"
      },
      "source": [
        "### 가장 기본적인 전처리를 해주는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TlehzefnA_Ef"
      },
      "outputs": [],
      "source": [
        "def preprocess_new_sentence(sent):\n",
        "  sent = re.sub('ㅜ', 'ㅠ', sent)\n",
        "  sent = re.sub('[^ㅋㅎㅡㅉㅠ 가-힣]', '', sent)\n",
        "  sent = emoticon_normalize(sent, num_repeats=2)\n",
        "  sent = re.sub('ㅋㅋㅋ','ㅋㅋ',sent)\n",
        "  sent = re.sub('ㅎㅎㅎ','ㅎㅎ',sent)\n",
        "  sent = re.sub('ㅠㅠㅠ','ㅠㅠ',sent)\n",
        "  sent = re.sub('ㅡㅡㅡ','ㅡㅡ',sent)\n",
        "  sent = re.sub('ㅉㅉㅉ','ㅉㅉ',sent)\n",
        "  sent = spell_checker.check(sent).checked\n",
        "  if sent=='':\n",
        "    return np.nan\n",
        "  return sent\n",
        "\n",
        "  # ''이 나올 수도 있다.(따로 잘 처리하자)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5qELlVx0C1h"
      },
      "source": [
        "### 샘플 하나 당 하나의 문장이 오도록 만드는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iej3Mnq7BGx7"
      },
      "outputs": [],
      "source": [
        "# 문장 여러 개를 각각 하나의 문장으로 분리해줄 함수 정의\n",
        "# 주의: dataframe의 인덱스가 오름차순읋 정렬되어 있어야 한다.\n",
        "def trans_df(df):\n",
        "  temp = pd.DataFrame({'document':[]})\n",
        "  num = 0\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    for s in sent_tokenize(df['document'][i]):\n",
        "      temp.loc[num] = [s]\n",
        "      num = num+1\n",
        "  return temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q8ClwW8BHBb"
      },
      "source": [
        "# 2. 학습된 모델을 이용해 예측하는 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe6jzy--BHEu"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V4iaU253mL3A"
      },
      "outputs": [],
      "source": [
        "with open(path+'tokenizer.pickle', 'rb') as handle:\n",
        "  tokenizer1 = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9_xFsrnJqogh"
      },
      "outputs": [],
      "source": [
        "loaded_lstm_model = load_model(path+'best_lstm_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jqOojIVwBHH-"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict_lstm(sentence):\n",
        "  sentece = preprocess_new_sentence(sentence)\n",
        "  okt = Okt()\n",
        "  tokenized_sentence = okt.pos(sentence, stem=True) # 토큰화\n",
        "  stopwords_removed_sentence = [word for word, pos in tokenized_sentence if (not word in ['ㅋ', 'ㅎ', 'ㅠ', 'ㅉ', 'ㅡ']) and (not pos in ['Josa', 'Eomi', 'PreEomi', 'Suffix', 'Unknown'])] # 불용어 제거\n",
        "  encoded = tokenizer1.texts_to_sequences([stopwords_removed_sentence])\n",
        "  if encoded==[[]]:\n",
        "    return np.nan\n",
        "  pad_new = pad_sequences(encoded, maxlen = 25) # 패딩\n",
        "  score = float(loaded_lstm_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    return 1    # 긍정\n",
        "  else:\n",
        "    return 0    # 부정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qc1oEvrZBHXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7048e75-69d3-4daa-b07d-1aa0089954c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "sentiment_predict_lstm('무조건 듣지 마세요 ㅋㅋ'), sentiment_predict_lstm('정말 기억에 남는 강의입니다!!!') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi80T8edBHZ7"
      },
      "source": [
        "### BI-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ExvJJgmdBHcr"
      },
      "outputs": [],
      "source": [
        "loaded_bilstm_model = load_model(path+'best_bilstm_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sF6X6SaSBHfc"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict_bilstm(sentence):\n",
        "  sentece = preprocess_new_sentence(sentence)\n",
        "  okt = Okt()\n",
        "  tokenized_sentence = okt.pos(sentence, stem=True) # 토큰화\n",
        "  stopwords_removed_sentence = [word for word, pos in tokenized_sentence if (not word in ['ㅋ', 'ㅎ', 'ㅠ', 'ㅉ', 'ㅡ']) and (not pos in ['Josa', 'Eomi', 'PreEomi', 'Suffix', 'Unknown'])] # 불용어 제거\n",
        "  encoded = tokenizer1.texts_to_sequences([stopwords_removed_sentence])\n",
        "  if encoded==[[]]:\n",
        "    return np.nan\n",
        "  pad_new = pad_sequences(encoded, maxlen = 25) # 패딩\n",
        "  score = float(loaded_bilstm_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    return 1    # 긍정\n",
        "  else:\n",
        "    return 0    # 부정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xd_9_1_SBHiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e77f61d-a498-484e-e56e-818fca8e6a09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "sentiment_predict_bilstm('무조건 듣지 마세요 ㅋㅋ'), sentiment_predict_bilstm('정말 기억에 남는 강의입니다!!!') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdFIVbEOBHk4"
      },
      "source": [
        "### GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "dewTgfdUv-3s"
      },
      "outputs": [],
      "source": [
        "# 구현된 모델 클래스에 학습된 파라미터를 다운로드\n",
        "import wget\n",
        "import zipfile\n",
        "\n",
        "wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')\n",
        "with zipfile.ZipFile('gpt_ckpt.zip') as z:\n",
        "    z.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "DlhWi9Kwv-7q"
      },
      "outputs": [],
      "source": [
        "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "tokenizer2 = SentencepieceTokenizer(TOKENIZER_PATH, num_best=-1) # 여기서 num_best를 반드시 설정해줘야 밑에서 오류 안남\n",
        "vocab2 = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH, \n",
        "                                               mask_token=None, \n",
        "                                               sep_token='<unused0>', \n",
        "                                               cls_token=None, \n",
        "                                               unknown_token='<unk>',  \n",
        "                                               padding_token='<pad>', \n",
        "                                               bos_token='<s>', \n",
        "                                               eos_token='</s>')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(tf.keras.Model):\n",
        "    def __init__(self, dir_path):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.gpt2(inputs)[0]"
      ],
      "metadata": {
        "id": "xLQ9d_9i1w9O"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TFGPT2Classifier(tf.keras.Model):\n",
        "    def __init__(self, dir_path, num_class):\n",
        "        super(TFGPT2Classifier, self).__init__()\n",
        "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
        "        self.num_class = num_class\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)\n",
        "        self.classifier = tf.keras.layers.Dense(self.num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range), name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        outputs = self.gpt2(inputs)\n",
        "        gpt_output =outputs[0][:, -1]\n",
        "\n",
        "        pooled_output = self.dropout(gpt_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "fItGyDyn1xU9"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqxfpcz921o6",
        "outputId": "e22ed3f6-15fb-4744-e2b3-0e73ecfb15d4"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_gpt2_model = TFGPT2Classifier(dir_path=BASE_MODEL_PATH, num_class=2)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(6.25e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "loaded_gpt2_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23oypsZY2md_",
        "outputId": "e79ce6b9-a5e9-498e-8e08-05dd80d7a3df"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at ./gpt_ckpt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 25 # okt 형태소 분석기로 분석했을 때 95%의 데이터들이 살아남은 지점의 길이\n",
        "NUM_EPOCHS = 1 # 임의로 설정\n",
        "BATCH_SIZE = 64 # 임의로 설정\n",
        "VALID_SPLIT = 0.1 # 임의로 설정"
      ],
      "metadata": {
        "id": "qq3_3UMO7J45"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(path + 'final_data.xlsx', engine='openpyxl')"
      ],
      "metadata": {
        "id": "il80OgP69JO3"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[:100]"
      ],
      "metadata": {
        "id": "ShcBuRTKD6Vh"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_sents = []\n",
        "train_data_labels = []\n",
        "\n",
        "for train_sent, train_label in data[['document', 'label']].values:\n",
        "    train_tokenized_text = vocab2[tokenizer2(train_sent)] \n",
        "    # 위 코드 우측 부분이 책에는 clean_text(train_sent)로 되어있었으나 검색해보니\n",
        "    # clean_text가 불필요한 문자 제거 그런거여서 이미 정제된 상태이므로 그냥 train_sent로 작성함\n",
        "\n",
        "    tokens = [vocab2[vocab2.bos_token]]\n",
        "    tokens += pad_sequences([train_tokenized_text], \n",
        "                            maxlen=MAX_LEN, # 앞에 6-2에서 임의로 정함\n",
        "                            value=vocab2[vocab2.padding_token], \n",
        "                            padding='pre').tolist()[0]\n",
        "    tokens += [vocab2[vocab2.eos_token]]\n",
        "\n",
        "    train_data_sents.append(tokens)\n",
        "    train_data_labels.append(train_label)"
      ],
      "metadata": {
        "id": "V2zgdYJm_jJ8"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_sents = np.array(train_data_sents, dtype=np.int64)\n",
        "train_data_labels = np.array(train_data_labels, dtype=np.int64)"
      ],
      "metadata": {
        "id": "BcAe3nYq_jPf"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습을 진행할 때는 GPU로 학습하는 것을 권장한다고 함(그래서 처음 코드 실행할 때부터 런타임:GPU로 해놓았음)\n",
        "model_name = 'tf2_gpt2_mark1'\n",
        "\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
        "\n",
        "# 이부분 경로는 일단 그냥 구글드라이브의 data 모아놨던 곳으로 코드 바꿈\n",
        "checkpoint_path = os.path.join(path, model_name, 'weights1.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else: \n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, \n",
        "    monitor=\"val_accuracy\", \n",
        "    verbose=1, \n",
        "    save_best_only=True, \n",
        "    save_weights_only=True, \n",
        ")\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = loaded_gpt2_model.fit(\n",
        "        train_data_sents, \n",
        "        train_data_labels, \n",
        "        epochs=NUM_EPOCHS, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        validation_split=VALID_SPLIT, \n",
        "        callbacks=[earlystop_callback, cp_callback] \n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwC48Aw4AU7C",
        "outputId": "b09073e2-f323-45cc-e0a3-8e23995d34da"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/FINAL_SUBMIT/tf2_gpt2_mark1 -- Folder already exists \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - ETA: 0s - loss: 0.8120 - accuracy: 0.0222     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to /content/gdrive/MyDrive/FINAL_SUBMIT/tf2_gpt2_mark1/weights1.h5\n",
            "2/2 [==============================] - 25s 9s/step - loss: 0.8120 - accuracy: 0.0222 - val_loss: 0.7679 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_gpt2_model.load_weights(path+'tf2_gpt2_mark1_final/weights.h5')"
      ],
      "metadata": {
        "id": "K4MW64NH1xaE"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "65FufoWlv-_W"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict_gpt2(sentence):\n",
        "  train_tokenized_text = vocab2[tokenizer2(sentence)] \n",
        "  tokens = [vocab2[vocab2.bos_token]]\n",
        "  tokens += pad_sequences([train_tokenized_text], \n",
        "                          maxlen=25,\n",
        "                          value=vocab2[vocab2.padding_token], \n",
        "                          padding='pre').tolist()[0]\n",
        "  tokens += [vocab2[vocab2.eos_token]]\n",
        "  score = loaded_gpt2_model.predict([tokens]) # 예측\n",
        "  return np.argmax(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "niIWE_Vzv_Gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429fb538-5351-455e-e589-bd1ec5820c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "sentiment_predict_gpt2('교수님 너무 좋아요 말씀도 친절하시고 최고 최고'), sentiment_predict_gpt2('교수님 수업에 열정도 있으시고 격려와 재미난 학습으로 즐겁게 배웠습니다\t') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KoBERT"
      ],
      "metadata": {
        "id": "mRg6CVzsv_VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
        "                 pad, pair):\n",
        "   \n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
        "        \n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "tOuw7fhPQUkF"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 2, \n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "AlwK90CXFJdo"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CPU사용시\n",
        "#device = torch.device(\"cpu:0\")\n",
        "\n",
        "#GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "LePRFRqkFJhb"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_kobert_model = torch.load(path + 'KoBERT_감정분석2.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
        "#GPU사용 불가일 경우, map_location=torch.device('cpu') 해줘야 함\n",
        "loaded_kobert_model.load_state_dict(torch.load(path + 'model_state_dict2.pt'))  # state_dict를 불러 온 후, 모델에 저장"
      ],
      "metadata": {
        "id": "XX-5R-epFJlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ce6b55-bd5a-40dc-f7b8-ca4e75b9ab2d"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel, vocab3 = get_pytorch_kobert_model(cachedir=\".cache\")"
      ],
      "metadata": {
        "id": "zOgYnptFFJpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c67fd10-c308-4a1a-d759-070da223bd3d"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_v1.zip\n",
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer3 = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer3, vocab3, lower=False)"
      ],
      "metadata": {
        "id": "baEya6AnFJs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20906d1-f79a-49a1-abb0-ffb931b92655"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "Yg-tCT2Sv_ZF"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict_kobert(sentence):\n",
        "    data = [sentence, '1']\n",
        "    data_set = [data]\n",
        "    test_data = BERTDataset(data_set, 0, 1, tok, vocab3, 25, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128, num_workers=2)\n",
        "  \n",
        "\n",
        "    loaded_kobert_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "            token_ids = token_ids.long().to(device)\n",
        "            segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "            valid_length= valid_length\n",
        "            label = label.long().to(device)\n",
        "\n",
        "            out = loaded_kobert_model(token_ids, valid_length, segment_ids)\n",
        "            \n",
        "            for i in out:\n",
        "                logits = i\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                return np.argmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict_kobert('무조건 듣지 마세요 ㅋㅋ'), sentiment_predict_kobert('정말 기억에 남는 강의입니다!!!') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2w9k5UTR73w",
        "outputId": "cea997ab-c8df-41e9-bd0a-b17365329fb1"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 에브리타임 데이터 가져오기"
      ],
      "metadata": {
        "id": "eO1qTYExKa6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 로드"
      ],
      "metadata": {
        "id": "kBubzE7HHmHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data0 = pd.read_excel(path + 'et_data.xlsx', engine='openpyxl')"
      ],
      "metadata": {
        "id": "YuM7AhTDc0UX"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 샘플 하나당 하나의 문장이 나오도록 변환"
      ],
      "metadata": {
        "id": "dMYR-reHIKvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data1 = eval_data0.copy()"
      ],
      "metadata": {
        "id": "Tj-zf_J_c0Xu"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data1 = trans_df(eval_data1)"
      ],
      "metadata": {
        "id": "2wwQG6jUc0ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "375093a3-30ba-4e4d-f5a6-80bcc082437b"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 129/129 [00:00<00:00, 330.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모든 샘플에 가장 기본적인 전처리"
      ],
      "metadata": {
        "id": "4AsZQxVkIYrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data2 = eval_data1.copy()"
      ],
      "metadata": {
        "id": "m7b3cOzYc0dt"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data2['document'] = eval_data2['document'].map(preprocess_new_sentence)"
      ],
      "metadata": {
        "id": "21OOOtdHc0go"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data2.isnull().sum()\n",
        "\n",
        "# 전처리 과정에서 결측값 발생"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaWtgbyLaWTs",
        "outputId": "69a5821a-8bfa-431a-a53b-f00b2b269868"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "document    4\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결측값 삭제"
      ],
      "metadata": {
        "id": "ClJ-ZVN_JBD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3 = eval_data2.copy()"
      ],
      "metadata": {
        "id": "h09Dolj-bBuv"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3 = eval_data3.dropna(how='any')"
      ],
      "metadata": {
        "id": "uSqqCctcb2Cx"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 앞서 학습한 모델 4개를 이용하여, 전처리한 에브리타임 데이터 예측"
      ],
      "metadata": {
        "id": "-95NxGnUJK1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3['prediction_lstm'] = eval_data3['document'].map(sentiment_predict_lstm)"
      ],
      "metadata": {
        "id": "ckQaQvrTb7Yx"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3['prediction_bilstm'] = eval_data3['document'].map(sentiment_predict_bilstm)"
      ],
      "metadata": {
        "id": "KMXz9YOrcOGG"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3['prediction_gpt2'] = eval_data3['document'].map(sentiment_predict_gpt2)"
      ],
      "metadata": {
        "id": "k_a7AUXrcoJV"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3['prediction_kobert'] = eval_data3['document'].map(sentiment_predict_kobert)"
      ],
      "metadata": {
        "id": "1JEIoJm-coNF"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3    # 데이터 프레임 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fG3WuinWcoQY",
        "outputId": "91e41ef2-3e92-4c1d-cad3-b3814d2f4f37"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              document  prediction_lstm  \\\n",
              "0    기말 팀 프로젝트가 제일 중요해요 다른 거 점수 잘 받아봤자 팀원 운이 진짜 좋아야...                1   \n",
              "1                           교수님이 친절하시고 학생에게 배려를 많이 줍니다                1   \n",
              "2               그래도 팀원을 잘 만나야 한 학기 머리 아프지 않게 지낼 수 있습니다                0   \n",
              "3    아니면 지잔 빡세게 해야 하고 머리만 아파요 ㅠㅠ 구래도 개인과제를 잘 하고 열심히...                1   \n",
              "4    들어보면 왜 평점이 좋은 수업인지 알 수 있습니다 학생 배려를 정말 많이 해주십니다...                0   \n",
              "..                                                 ...              ...   \n",
              "233                비대면 수업이었는데 교수님 좋으시고 학생들 배려 많이 해주셨어요                1   \n",
              "234                              피드백도 꼼꼼히 잘 해주시고 친절하세요                1   \n",
              "235          하지만 액트는 팀은 빨 매주 또는 주에 한 번 정도 팀 과제가 계속 있어요                0   \n",
              "236            교수님 수업에 열정도 있으시고 격려와 재미난 학습으로 즐겁게 배웠습니다                1   \n",
              "237                         교수님 너무 좋아요 말씀도 친절하시고 최고 최고                1   \n",
              "\n",
              "     prediction_bilstm  prediction_gpt2  prediction_kobert  \n",
              "0                    1                1                  1  \n",
              "1                    1                1                  1  \n",
              "2                    0                1                  0  \n",
              "3                    0                1                  0  \n",
              "4                    0                0                  1  \n",
              "..                 ...              ...                ...  \n",
              "233                  1                1                  1  \n",
              "234                  1                1                  1  \n",
              "235                  0                1                  1  \n",
              "236                  1                1                  1  \n",
              "237                  1                1                  1  \n",
              "\n",
              "[234 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-945f3e8d-dfdd-44d2-82c0-64189522168b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>prediction_lstm</th>\n",
              "      <th>prediction_bilstm</th>\n",
              "      <th>prediction_gpt2</th>\n",
              "      <th>prediction_kobert</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>기말 팀 프로젝트가 제일 중요해요 다른 거 점수 잘 받아봤자 팀원 운이 진짜 좋아야...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>교수님이 친절하시고 학생에게 배려를 많이 줍니다</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>그래도 팀원을 잘 만나야 한 학기 머리 아프지 않게 지낼 수 있습니다</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>아니면 지잔 빡세게 해야 하고 머리만 아파요 ㅠㅠ 구래도 개인과제를 잘 하고 열심히...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>들어보면 왜 평점이 좋은 수업인지 알 수 있습니다 학생 배려를 정말 많이 해주십니다...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>비대면 수업이었는데 교수님 좋으시고 학생들 배려 많이 해주셨어요</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>피드백도 꼼꼼히 잘 해주시고 친절하세요</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>하지만 액트는 팀은 빨 매주 또는 주에 한 번 정도 팀 과제가 계속 있어요</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>교수님 수업에 열정도 있으시고 격려와 재미난 학습으로 즐겁게 배웠습니다</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>교수님 너무 좋아요 말씀도 친절하시고 최고 최고</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>234 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-945f3e8d-dfdd-44d2-82c0-64189522168b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-945f3e8d-dfdd-44d2-82c0-64189522168b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-945f3e8d-dfdd-44d2-82c0-64189522168b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data3.to_excel(path+'prediction_et_data.xlsx', index=False)    # 예측한 데이터 저장"
      ],
      "metadata": {
        "id": "fNVlctjfcoUK"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 강의 점수 산출"
      ],
      "metadata": {
        "id": "HJ63_W_yKPrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('ACT')\n",
        "print('캠퍼스: 서울캠퍼스')\n",
        "print('교수명: ###\\n')\n",
        "for model in ['lstm', 'bilstm', 'gpt2', 'kobert']:\n",
        "  print('{} 모델을 이용해 산출한 강의 점수: {:.1f}점'.format(model, eval_data3[f'prediction_{model}'].sum()/eval_data3.shape[0]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAle8fVmcoXS",
        "outputId": "23a63ed7-c767-4e8e-fc34-99983190ff96"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACT\n",
            "캠퍼스: 서울캠퍼스\n",
            "교수명: ###\n",
            "\n",
            "lstm 모델을 이용해 산출한 강의 점수: 66.7점\n",
            "bilstm 모델을 이용해 산출한 강의 점수: 67.9점\n",
            "gpt2 모델을 이용해 산출한 강의 점수: 73.5점\n",
            "kobert 모델을 이용해 산출한 강의 점수: 73.9점\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "q-tblwjkCvYt",
        "R6mM--UVz1hi",
        "Qe6jzy--BHEu",
        "Hi80T8edBHZ7",
        "qdFIVbEOBHk4",
        "mRg6CVzsv_VE"
      ],
      "name": "EVERYTIME_DATA_PREDICTING",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}